{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NOTE_SIZE = 128\n",
    "DUR_SIZE = 160\n",
    "TIM_SIZE = 1000\n",
    "VEL_SIZE = 128\n",
    "\n",
    "\n",
    "NOTE_TOKS = [f'n{i}' for i in range(NOTE_SIZE)] \n",
    "DUR_TOKS = [f'd{i}' for i in range(DUR_SIZE)]\n",
    "TIM_TOKS = [f't{i}' for i in range(TIM_SIZE)]\n",
    "VEL_TOKS = [f'v{i}' for i in range(VEL_SIZE)]\n",
    "\n",
    "VOCAB = NOTE_TOKS + DUR_TOKS + TIM_TOKS + VEL_TOKS\n",
    "\n",
    "DICT = [(element, index) for index, element in enumerate(VOCAB)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "\n",
    "custom_vocab = vocab(OrderedDict(DICT))\n",
    "itos_vocab = custom_vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BennyGoodman_TigerRag-1_FINAL.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BennyGoodman_TigerRag-2_FINAL.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BennyGoodman_Whispering_FINAL.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  3.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import os\n",
    "\n",
    "\n",
    "# Load the MIDI file\n",
    "midi_file = midi.MidiFile()\n",
    "\n",
    "les_tokens = []\n",
    "\n",
    "folder_path = \"midi_data\"  \n",
    "\n",
    "# Get all the file names in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "for f in tqdm(file_names):\n",
    "    print(f)\n",
    "    midi_file = midi.MidiFile()\n",
    "    midi_file.open(\"midi_data/\" +f)\n",
    "    midi_file.read()\n",
    "    midi_file.close()\n",
    "    # Create a stream from the MIDI file\n",
    "    stream = midi.translate.midiFileToStream(midi_file)\n",
    "\n",
    "    # Iterate over the notes in the stream and extract the note information\n",
    "    last_time = 0\n",
    "\n",
    "    for note in stream.flat.notes:\n",
    "        if note.isNote:\n",
    "            note_pitch = note.pitch.midi\n",
    "            # A terme il faudra arrondir plutot que de prendre la partie entiere\n",
    "            note_duration = int(note.duration.quarterLength*4)\n",
    "            note_offset = int(note.offset*4 - last_time)\n",
    "            last_time = note.offset*4\n",
    "            note_velocity = note.volume.velocity\n",
    "            les_tokens.append(NOTE_TOKS[note_pitch])\n",
    "            les_tokens.append(DUR_TOKS[note_duration])\n",
    "            les_tokens.append(TIM_TOKS[note_offset])\n",
    "            les_tokens.append(VEL_TOKS[note_velocity])\n",
    "            # print(\"Note Pitch:\", note_pitch)\n",
    "            # print(\"Note Duration:\", note_duration)\n",
    "            # print(\"Note TimeShift:\", note_offset)\n",
    "            # print(\"Note Velocity:\", note_velocity)\n",
    "\n",
    "        if note.isChord:\n",
    "\n",
    "            for note2 in note:\n",
    "                note_pitch = note2.pitch.midi\n",
    "                note_duration = int(note.duration.quarterLength*4)\n",
    "                note_offset = int(note.offset*4 - last_time)\n",
    "                last_time = note.offset*4\n",
    "                note_velocity = note2.volume.velocity\n",
    "                les_tokens.append(NOTE_TOKS[note_pitch])\n",
    "                les_tokens.append(DUR_TOKS[note_duration])\n",
    "                les_tokens.append(TIM_TOKS[note_offset])\n",
    "                les_tokens.append(VEL_TOKS[note_velocity])\n",
    "                # print(\"Note Pitch:\", note_pitch)\n",
    "                # print(\"Note Duration:\", note_duration)\n",
    "                # print(\"Note Time:\", note_offset)\n",
    "                # print(\"Note Velocity:\", note_velocity)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#répartir le data du morceau en blocs de 120 attributs (30 notes)\n",
    "#Et associer à chaque bloc la réponse attendue (l'attribut suivant)\n",
    "\n",
    "taille_bloc = 120\n",
    "les_morceaux = []\n",
    "les_morceaux_shift = []\n",
    "\n",
    "for i in range(len(les_tokens)//(taille_bloc+1)-1):\n",
    "    les_morceaux.append(les_tokens[i:i+taille_bloc])\n",
    "    les_morceaux_shift.append(les_tokens[i+1:i+1+taille_bloc])\n",
    "\n",
    "\n",
    "def vectorize(i):\n",
    "    return [0]*i + [1] + [0]* (len(custom_vocab)-i-1)\n",
    "def unvectorize(v):\n",
    "    for i in range(len(v)):\n",
    "        if v[i]:\n",
    "            return i\n",
    "\n",
    "input_vect = [ [vectorize(custom_vocab[tok]) for tok in morceau] for morceau in les_morceaux ]\n",
    "rep_vect = [ [vectorize(custom_vocab[tok]) for tok in morceau] for morceau in les_morceaux_shift ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Batch   0, Loss: 9.7323\n",
      "Epoch   1, Batch   0, Loss: 10.3755\n",
      "Epoch   2, Batch   0, Loss: 9.4902\n",
      "Epoch   3, Batch   0, Loss: 10.2703\n",
      "Epoch   4, Batch   0, Loss: 10.0086\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from transformers import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "num_tokens = len(custom_vocab)\n",
    "embed_dim = len(custom_vocab) #la taille du voc car on fait du one hot\n",
    "num_heads = 283\n",
    "hidden_dim = 64\n",
    "dropout = 0.1\n",
    "\n",
    "# Define the Transformer model\n",
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        self.decoder = torch.nn.Linear(embed_dim, num_tokens)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, memory):\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.transformer_decoder(x, memory)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-init_range, init_range)\n",
    "        \n",
    "# Define the dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, input_data, output_data):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data    \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.tensor(self.input_data[idx], dtype=torch.long)\n",
    "        output_tensor = torch.tensor(self.output_data[idx], dtype=torch.long)\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "dataset = MyDataset(input_vect, rep_vect)\n",
    "\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel()\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        inp, rep  = batch\n",
    "        inp = inp.to(torch.float32)\n",
    "        rep = rep.to(torch.float32)\n",
    "        # Create fake encoder outputs\n",
    "        memory = torch.zeros(batch_size,len(inp[0]), embed_dim).to(inp.device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inp, memory)\n",
    "        output_prob = torch.softmax(output, dim=2)\n",
    "        # Flatten the batch and target tensors for use in the loss function\n",
    "        output_flat = output[0]\n",
    "\n",
    "        loss = loss_fn(output_flat, rep[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\"Epoch {:3d}, Batch {:3d}, Loss: {:.4f}\".format(epoch, batch_idx, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.tensor(input_vect[15], dtype=torch.long).unsqueeze(0) # unsqueeze adds a batch dimension\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_vocab[les_morceaux[15][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  58,   58,   58,   58,   58,   58,   58,   58,   62,   55,   58,   58,\n",
       "          58, 1387,   58,   58,   58,   58,   58,   58,   58,   58,   58,   58,\n",
       "          58, 1385,   62, 1385,   58, 1394,   58,   58,   62,   58,   58,   58,\n",
       "          58,   58,   58,   58,   58, 1387,   58,   58,   58,   58,   58,  292,\n",
       "          58,   58,   58,   62,  130,   62,   62,   62,  292,   62,   62,   58,\n",
       "         292, 1385,  292, 1385,   58, 1394, 1385, 1394,   58,   58,   58,   58,\n",
       "          58,   58,   58,   58,   62,   58,   58,  292,   58,   58,   62,   58,\n",
       "          62,   58,   58,   62,   58,   58,   58,   62,   58,   58, 1385,   58,\n",
       "          58,   58,   58,   58,   58,   62,   58,   58, 1385, 1394,   58,   58,\n",
       "          58,   58, 1385,   58,   58,   58,   58,   58,   58, 1385,   55,   58])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = torch.zeros(1, example.size(1), embed_dim)\n",
    "output = model(example, memory)\n",
    "predicted_tokens = torch.argmax(output[0], dim=1)\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v116',\n",
       " 'n59',\n",
       " 'd4',\n",
       " 't4',\n",
       " 'v98',\n",
       " 'n60',\n",
       " 'd4',\n",
       " 't5',\n",
       " 'v100',\n",
       " 'n63',\n",
       " 'd3',\n",
       " 't4',\n",
       " 'v104',\n",
       " 'n63',\n",
       " 'd1',\n",
       " 't3',\n",
       " 'v104',\n",
       " 'n59',\n",
       " 'd2',\n",
       " 't2',\n",
       " 'v91',\n",
       " 'n60',\n",
       " 'd4',\n",
       " 't3',\n",
       " 'v98',\n",
       " 'n63',\n",
       " 'd4',\n",
       " 't3',\n",
       " 'v109',\n",
       " 'n59',\n",
       " 'd2',\n",
       " 't4',\n",
       " 'v96',\n",
       " 'n59',\n",
       " 'd1',\n",
       " 't2',\n",
       " 'v96',\n",
       " 'n60',\n",
       " 'd4',\n",
       " 't2',\n",
       " 'v100',\n",
       " 'n65',\n",
       " 'd3',\n",
       " 't3',\n",
       " 'v106',\n",
       " 'n59',\n",
       " 'd4',\n",
       " 't4',\n",
       " 'v98',\n",
       " 'n60',\n",
       " 'd2',\n",
       " 't4',\n",
       " 'v98',\n",
       " 'n60',\n",
       " 'd2',\n",
       " 't2',\n",
       " 'v98',\n",
       " 'n63',\n",
       " 'd4',\n",
       " 't2',\n",
       " 'v101',\n",
       " 'n53',\n",
       " 'd3',\n",
       " 't4',\n",
       " 'v100',\n",
       " 'n51',\n",
       " 'd3',\n",
       " 't4',\n",
       " 'v92',\n",
       " 'n56',\n",
       " 'd2',\n",
       " 't4',\n",
       " 'v99',\n",
       " 'n56',\n",
       " 'd4',\n",
       " 't2',\n",
       " 'v99',\n",
       " 'n51',\n",
       " 'd2',\n",
       " 't5',\n",
       " 'v101',\n",
       " 'n53',\n",
       " 'd1',\n",
       " 't3',\n",
       " 'v86',\n",
       " 'n56',\n",
       " 'd2',\n",
       " 't1',\n",
       " 'v107',\n",
       " 'n60',\n",
       " 'd1',\n",
       " 't3',\n",
       " 'v105',\n",
       " 'n63',\n",
       " 'd2',\n",
       " 't1',\n",
       " 'v105',\n",
       " 'n63',\n",
       " 'd2',\n",
       " 't2',\n",
       " 'v105',\n",
       " 'n51',\n",
       " 'd2',\n",
       " 't2',\n",
       " 'v118',\n",
       " 'n53',\n",
       " 'd1',\n",
       " 't2',\n",
       " 'v98',\n",
       " 'n56',\n",
       " 'd2',\n",
       " 't1',\n",
       " 'v101',\n",
       " 'n60',\n",
       " 'd1',\n",
       " 't2',\n",
       " 'v107',\n",
       " 'n63',\n",
       " 'd4',\n",
       " 't1']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itos_vocab[unvectorize(el)] for el in input_vect[15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n56',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v100',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v98',\n",
       " 'n63',\n",
       " 'v98',\n",
       " 'n59',\n",
       " 'v107',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v100',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 't5',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'd3',\n",
       " 'n63',\n",
       " 'n63',\n",
       " 'n63',\n",
       " 't5',\n",
       " 'n63',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 't5',\n",
       " 'v98',\n",
       " 't5',\n",
       " 'v98',\n",
       " 'n59',\n",
       " 'v107',\n",
       " 'v98',\n",
       " 'v107',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 't5',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v98',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n63',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v98',\n",
       " 'v107',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v98',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'n59',\n",
       " 'v98',\n",
       " 'n56',\n",
       " 'n59']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itos_vocab[el] for el in predicted_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicautobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a8b4db5274a6d0c2db0e0a615c1726b63cd4b0971dd8ff6593a5ffc2178cc4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
